{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1.- Mini-análisis usando el modelo final**"
      ],
      "metadata": {
        "id": "eGR1wNmhOVjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Sube el archivo wind_ava.csv desde tu ordenador\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# También es una opción montar el drive para incluir el archivo desde ahí\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-SwOCGUiO9kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8d397f-c986-4ae7-d474-069f3d40d67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import re\n",
        "\n",
        "# Estos son datos que vamos a usar para todos los modelos\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/AA_P1_Grupo10/wind_ava.csv\")\n",
        "\n",
        "patron = r'.*\\.13$'\n",
        "columnas_a_mantener = []\n",
        "for column in df.columns:\n",
        "  if column == \"datetime\" or column == \"energy\":\n",
        "    columnas_a_mantener.append(column)\n",
        "  if re.match(patron, column):\n",
        "    columnas_a_mantener.append(column)\n",
        "df = df[columnas_a_mantener]\n",
        "\n",
        "columnas_a_eliminar = ['energy', 'datetime', 'p55.162.13', 'stl4.13', 'cape.13']\n",
        "y = df['energy']\n",
        "X = df.drop(columns=columnas_a_eliminar)\n",
        "tscv = TimeSeriesSplit(n_splits=3)"
      ],
      "metadata": {
        "id": "E9VGJ5XePHLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "modelo_lasso = load('/content/drive/MyDrive/AA_P1_Grupo10/modelo_final.pkl')\n",
        "\n",
        "\n",
        "\n",
        "# Supongamos que tienes tus datos X y y\n",
        "\n",
        "# Divide tus datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrena tu modelo en el conjunto de entrenamiento\n",
        "\n",
        "# Realiza predicciones en el conjunto de prueba\n",
        "y_pred = modelo_lasso.predict(X_test)\n",
        "\n",
        "# Crea diferentes rangos de valores de la variable objetivo\n",
        "# Por ejemplo, podrías dividir los valores en cuartiles\n",
        "quantiles = np.percentile(y_test, [25, 50, 75])\n",
        "\n",
        "# Calcula las métricas de rendimiento para cada rango\n",
        "mae_by_quantile = []\n",
        "for i in range(len(quantiles)):\n",
        "    if i == 0:\n",
        "        mask = (y_test <= quantiles[i])\n",
        "    elif i == len(quantiles) - 1:\n",
        "        mask = (y_test > quantiles[i-1])\n",
        "    else:\n",
        "        mask = (y_test > quantiles[i-1]) & (y_test <= quantiles[i])\n",
        "    mae = mean_squared_error(y_test[mask], y_pred[mask])\n",
        "    mae_by_quantile.append(mae)\n",
        "\n",
        "# Imprime las métricas de rendimiento para cada rango\n",
        "for i, mae in enumerate(mae_by_quantile):\n",
        "    print(f\"MSE for quantile {i+1}: {np.sqrt(mae)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O4LBOSgF6dH",
        "outputId": "a09880bf-de10-4499-aace-a268b756caa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for quantile 1: 258.9253210844432\n",
            "MSE for quantile 2: 264.24999874017374\n",
            "MSE for quantile 3: 437.59340039440224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.- Conversión a un problema de clasificación**"
      ],
      "metadata": {
        "id": "Do9umGrCOFSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Calculamos el tercer cuantil\n",
        "tercer_cuantil = np.percentile(y, 75)\n",
        "print(\"Tercer cuantil:\", tercer_cuantil)\n",
        "# Creamos la nueva y\n",
        "new_y = np.where(y > tercer_cuantil, 'alta', 'baja')\n",
        "\n",
        "#smote = SMOTE()\n",
        "#rus = RandomUnderSampler(random_state=42)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Aplicar SMOTE a tus datos\n",
        "#X, new_y = smote.fit_resample(X, new_y)\n",
        "X, new_y = ros.fit_resample(X, new_y)\n",
        "#X, new_y = rus.fit_resample(X, new_y)\n"
      ],
      "metadata": {
        "id": "JdkmGlIUPJ3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2ce6bf-148d-4f4a-efc5-5d13355b5667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tercer cuantil: 1089.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.- Entrenamiento y selección de modelos**"
      ],
      "metadata": {
        "id": "fhN4JKkzOK0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - KNN"
      ],
      "metadata": {
        "id": "tDhNLWo0POko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "param_grid = {\n",
        "    'kneighborsclassifier__n_neighbors': list(range(2, 30, 2)),\n",
        "    'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
        "    'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
        "    'kneighborsclassifier__p': [1, 2]  # Solo se aplica si se usa la distancia de Minkowski\n",
        "}\n",
        "knn_model_hpo = make_pipeline(RobustScaler(), KNeighborsClassifier())\n",
        "\n",
        "# Hacemos nested cross validation\n",
        "# Hacemos la evaluacion inner usando un RandomizedSearch con un budget de 20\n",
        "regr = RandomizedSearchCV(estimator=knn_model_hpo, param_distributions=param_grid,\n",
        "                                   n_iter=20, scoring='accuracy',\n",
        "                                   cv=tscv, random_state=42)\n",
        "\n",
        "regr.fit(X, new_y)\n",
        "print(\"Best params\", regr.best_params_)\n",
        "\n",
        "# Hacemos la outer evaluation\n",
        "knn_scores_hpo = cross_val_score(regr, X, new_y, scoring='accuracy', cv = tscv)\n",
        "knn_accuracy_mean = knn_scores_hpo.mean()\n",
        "print(\"El accuracy medio es \", knn_accuracy_mean)"
      ],
      "metadata": {
        "id": "bTtMk6yMPWtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed2ab93-f6d7-4d6c-a113-dbccc9de9ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params {'kneighborsclassifier__weights': 'distance', 'kneighborsclassifier__p': 1, 'kneighborsclassifier__n_neighbors': 16, 'kneighborsclassifier__metric': 'manhattan'}\n",
            "El accuracy medio es  0.9138576779026217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Árboles de decisión"
      ],
      "metadata": {
        "id": "3nGmziMKPXWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn import tree\n",
        "\n",
        "tree_hpo = tree.DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {'max_depth': sp_randint (1, 16),\n",
        "              'min_samples_split': sp_randint (2, 40),\n",
        "              'min_samples_leaf': sp_randint (1, 20)}\n",
        "regr = RandomizedSearchCV(estimator=tree_hpo, param_distributions=param_grid,\n",
        "                                   n_iter=20, scoring='accuracy',\n",
        "                                   cv=tscv, random_state=42)\n",
        "\n",
        "regr.fit(X, new_y)\n",
        "print(\"Best params\", regr.best_params_)\n",
        "\n",
        "tree_scores_hpo = cross_val_score(regr,\n",
        "                            X, new_y,\n",
        "                            scoring='accuracy',\n",
        "                            cv = tscv)\n",
        "\n",
        "tree_accuracy_mean = tree_scores_hpo.mean()\n",
        "print(\"El accuracy medio es \", tree_accuracy_mean)"
      ],
      "metadata": {
        "id": "vHzryFnPPuWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c2d3f7-2f87-426a-a851-12c23881826a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params {'max_depth': 12, 'min_samples_leaf': 1, 'min_samples_split': 13}\n",
            "El accuracy medio es  0.8256554307116105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 - Regresión logística"
      ],
      "metadata": {
        "id": "V_6cgN6RPc5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import loguniform\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "log_reg_hpo = make_pipeline(RobustScaler(), LogisticRegression(max_iter=1000))\n",
        "\n",
        "param_grid = {#'logisticregression__penalty': ['l2', None],\n",
        "              'logisticregression__C': np.logspace(-4, 4, 20)}\n",
        "\n",
        "regr = RandomizedSearchCV(estimator=log_reg_hpo, param_distributions=param_grid,\n",
        "                                   n_iter=20, scoring='accuracy',\n",
        "                                   cv=tscv, random_state=42)\n",
        "regr.fit(X, new_y)\n",
        "print(\"Best params\", regr.best_params_)\n",
        "\n",
        "log_reg_hpo_scores = cross_val_score(regr, X, new_y, cv=tscv, scoring='accuracy')\n",
        "log_reg_accuracy_mean = log_reg_hpo_scores.mean()\n",
        "print(\"El accuracy medio es \", log_reg_accuracy_mean)\n",
        "\n",
        "warnings.filterwarnings(\"default\")"
      ],
      "metadata": {
        "id": "5ywzi9OjPwzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072d3cba-4960-486d-9b84-40fc20a2d37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params {'logisticregression__C': 11.288378916846883}\n",
            "El accuracy medio es  0.6692883895131084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 - SVM\n"
      ],
      "metadata": {
        "id": "5in3V37SPjtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "svm_reg_hpo = make_pipeline(RobustScaler(), SVC())\n",
        "\n",
        "param_grid = {'svc__C': loguniform(2**-5, 2**15),\n",
        "              'svc__gamma': loguniform(2**-15, 8),\n",
        "              'svc__kernel': [\"linear\"]}\n",
        "\n",
        "regr = RandomizedSearchCV(estimator=svm_reg_hpo, param_distributions=param_grid,\n",
        "                                   n_iter=20, scoring='accuracy',\n",
        "                                   cv=tscv, random_state=42)\n",
        "\n",
        "regr.fit(X, new_y)\n",
        "print(\"Best params\", regr.best_params_)\n",
        "\n",
        "svm_hpo_scores = cross_val_score(regr, X, new_y, cv=tscv, scoring='accuracy')\n",
        "svm_accuracy_mean = svm_hpo_scores.mean()\n",
        "print(\"El accuracy medio es \", svm_accuracy_mean)"
      ],
      "metadata": {
        "id": "cZkmTaVkP1Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 - Dummy Classifier"
      ],
      "metadata": {
        "id": "gJa4lhY1NqyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Crear el modelo DummyRegressor que predice la media\n",
        "dummy_model = DummyClassifier(strategy=\"most_frequent\")\n",
        "\n",
        "# Realizar la validación cruzada\n",
        "dummy_scores = cross_val_score(dummy_model, X, new_y, cv=2, scoring='accuracy')\n",
        "print(\"Error del modelo dummy:\", dummy_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ni_LsdONtsV",
        "outputId": "85050077-baa3-42ff-80f2-ace713686318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error del modelo dummy: 0.4962089300758214\n"
          ]
        }
      ]
    }
  ]
}